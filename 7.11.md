### Linear data generating process
$y_i = \beta x_{1i} + \beta_2x_{2i} + ... + \beta_jx_{ji} + error_i$

**To estimate with OLS, we assume (in past)**
1. $\mathbb{E}[error] = 0$
2. $\mathbb{Cov}(error_i, x_{ji}) = 0$ ie error is independent of covariates
3. $\mathbb{Var}(error_i | x_{ji}) = \mathbb{Var}(error_i)$ ie error term is homoskedastic 


### How to deal with heavy data?
1. Data transformation
2. Weighted regression

### What is Proxy Variableï¼Ÿ
some variables are not explanable. For example, how to measure "desire" or "confidence"? You can measure "how much thing people buy" as proxy for explanatory. 

### What is $R^2$?
\begin{equation}
R^2 = 1- \frac{RSS}{TSS} = 1 - \frac{\sum^n_i \hat{Error}^2}{\sum^n_i (y_i - \bar{y})^2}
\end{equation}

**$where:$**
$\hat{\mathbb{e}_i} = y_i - \hat{y}_i$
$\hat{y}_i = \hat{\beta_0} + \hat{\beta_1}x_{1i} + ... + \hat{\beta_j}x_{ji}$ $or$ $fitted$ $y_i$

$R^2$ = explained condition on a% of total conditions
$top$ = total unexplained variation
$bottom$ = total variation




## Some ML concepts
### What are Weights and Biases?
**weights** => how many weights are assigned to variables at each layer
**biases** => all ml has bias. Bias is the tendency to draw conclusions based on features
$r^2$ is weights and bias in ml
#### Ideal $R^2$ is $40$%
#### ==overfitting($R^2$ is too high)==: 
- NO bias. model run out of bias while evaluating the data. There are so many features and observations wrapped into the data that model drew too much connections between everything. The model has too much data, or too many data has been passed with inferior preprocessing.
#### ==underfitting($R^2$ is too low)==:
- TOO MUCH bias due to few examples. The model does not have enough data. If bias is high, but the variance is low, you need more data in your model. 

![](https://hackmd.io/_uploads/r1ieTjnK3.png)

